---
title: "RTX 3060で神を作る — 古事記アーキテクチャで挑んだJulia特化LLM開発記"
emoji: "⛩️"
type: "tech"
topics: ["Julia", "LLM", "アーキテクチャ", "Rust", "LLMエージェント"]
published: true
---

## はじめに：なぜ「神話」と「Julia」なのか

### 古事記に秘匿された、完全なる「ステート隔離」のアーキテクチャ

「L3（推論）とL5（評価）間の物理的隔離による、ハルシネーションの構造的封じ込めと、Juliaの型システムを利用したゼロ・ショットでの多重ディスパッチ解決。——これは、神話の皮を被った『計算機科学の聖域』の構築記録である。」


### なぜJuliaなのか

Juliaは科学技術計算の言語だ。Pythonのような書きやすさと、C並みの速度を両立する。そして何より、多重ディスパッチという独特の型システムを持つ。

「優れたローカルLLM」こそがJuliaのシェアを爆発させる最後の一ピースになる。

既存の（米中が誇る巨大な）LLMは、情報の海を漂うだけの「写経機」に過ぎない。しかし、Juliaの魂である型システムを理解し、「黄泉比良坂」という名の物理的ファイアウォールで推論と評価を峻別する私の設計は、ハルシネーションという名の「穢れ」を構造的に拒絶する。

この「型を識（し）る知能」は、Rustという峻烈な型世界をも鮮やかに攻略できる可能性がある。巨大な計算リソースで殴り合う時代を終わらせ、知能の「質」と「規律」で世界を再定義する。

```julia
# 同じ関数名でも、引数の型の組み合わせで挙動が変わる
add(x::Int, y::Int) = x + y          # 整数同士
add(x::Float64, y::Float64) = x + y  # 浮動小数点同士
add(x::String, y::String) = x * y    # 文字列は連結
```

この「型の組み合わせで現象が決まる」という構造を見て、ふと気づいた。

**これは古事記の神々と同じではないか。**

### 古事記との構造的類似性

古事記では、神々の「組み合わせ」によって世界が生まれる。イザナギとイザナミが出会えば国が生まれ、天照と須佐之男が対立すれば嵐が起きる。

```
Julia:   add(Int, Int) → 整数加算
古事記:  イザナギ + イザナミ → 国生み

Julia:   関数(型A, 型B) → 結果
古事記:  神A + 神B → 現象
```

この対応関係を活かせば、LLMの「黒魔術的な挙動」を、神話のシステムで**制御可能**にできるのではないか。

こうして、**Julia-no-Mikoto（ジュリアのミコト）** プロジェクトが始まった。

---

## 設計思想：神話がLLMに効く理由

### Transformerの各層を神話にマッピング

LLMの内部構造は、実は古事記の世界創造と驚くほど対応する。

| LLMの構造 | 古事記の概念 | 役割 |
|-----------|-------------|------|
| Embedding層 | 天地開闢（造化三神） | 無から有を生む |
| Attention層 | 神世七代（対の神々） | 関係性の発見 |
| FFN層 | 国生み | 具体的な生成 |
| Output層 | 三貴子（天照・月読・須佐之男） | 多様な出力 |
| Normalization | 禊（みそぎ） | 浄化・正規化 |
| エラー検出 | 黄泉の国 | 不安定性の検知 |

### なぜこのマッピングが有効なのか

単なる「名前の付け替え」ではない。神話の構造をアーキテクチャに反映させることで、以下の効果が得られた：

1. **MoE（Mixture of Experts）の自然な導入**: 四柱の神（天照・月読・須佐之男・思金）がそれぞれ専門性を持つ
2. **型システムの明示的モデリング**: 神産巣日神（カミムスヒ）が型階層を埋め込む
3. **エラー検出の体系化**: 黄泉の国が「型不安定」を検知し、禊で浄化する

---

## 66.5Mパラメータの小神

### モデル仕様

最終的に完成したモデルの仕様：

| 項目 | 値 |
|------|-----|
| パラメータ数 | 66.5M（MoE版） |
| コンテキスト長 | 2,048トークン |
| 語彙サイズ | 8,000トークン |
| 型語彙 | 128カテゴリ + 1,024ハッシュバケット |
| 必要VRAM | 約4GB（推論時） |

現代のLLM（GPT-4: 1.8T、Llama 3: 70B）と比べれば、文字通り**極小**だ。しかし、この制約の中で「古事記アーキテクチャ」がどこまで機能するかを検証することに意味があった。

### アーキテクチャ全体図

```
入力: Juliaコード
    │
    ▼
┌─────────────────────────────────────┐
│ 第一章: 天地開闢層 (Genesis)        │
│   天之御中主神: Positional Encoding │
│   高御産巣日神: Token Embedding     │
│   神産巣日神: Type Hierarchy Embed  │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 八咫鏡 Attention                    │
│   定義トークンへの選択的注視        │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 共有層 ×2 + MoE層 ×4                │
│   四柱: 天照/月読/須佐之男/思金     │
│   Top-2ルーティング                 │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 第三章: 国生み層 (Kuniumi)          │
│   3段階: struct → function → expr  │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 第四章: 黄泉国層 (Yomi)             │
│   型不安定検出・境界判定            │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 第五章: 禊層 (Misogi)               │
│   三貴子ヘッド:                     │
│   ├── 天照: 次トークン予測          │
│   ├── 月読: 次トークン型予測        │
│   └── 須佐之男: エラー予測          │
└─────────────────────────────────────┘
    │
    ▼
出力: トークン + 型 + 診断情報
```

---

## 学習データ：神々に捧げる供物

### 公開データセットの現状

Juliaのソースコードを含む大規模データセットは存在する：

| データセット | 規模 | 特徴 |
|-------------|------|------|
| The Stack (BigCode) | 6TB超 | 358言語、Julia含む |
| codeparrot/github-code | 大規模 | 約30言語 |
| CodeSearchNet | 中規模 | 自然言語クエリとのペア |

しかし、**古事記アーキテクチャに必要な情報は含まれていない**。

### 設計とのギャップ

| 必要な情報 | 公開データ | 解決策 |
|-----------|-----------|--------|
| 型階層ID（神産巣日神） | ❌ なし | JuliaSyntax.jlでAST生成→型推論 |
| 型安定性ラベル（黄泉国） | ❌ なし | @code_warntype相当の判定を付加 |
| SIMD/自動微分可能性（三貴子） | ❌ なし | 高性能ライブラリ使用箇所をタグ付け |

つまり、**「生のソースコード」を「型情報付きコード」に変換する前処理パイプライン**が必須だった。

### 独自データパイプラインの構築

最終的に、以下の15パッケージ（52MB）を収集し、独自の前処理を施した：

```
DataFrames.jl, Flux.jl, JuMP.jl, CSV.jl, HTTP.jl,
Plots.jl, DifferentialEquations.jl, Optim.jl, ...
```

**前処理フロー：**

```
.jlファイル収集（52MB）
    │
    ▼
JuliaSyntax.jl でトークン化
    │
    ▼
JET.jl で型推論・安定性判定
    │
    ▼
型ID / 具体度 / 階層深度 / 安定性ラベル を付与
    │
    ▼
PyTorch用 .pt ファイル（75MB）
```

この「型タグ付きデータセット」は、公開されているどのJuliaデータセットにもない、Julia-no-Mikoto固有の資産となった。

### トークナイザー

既存のLLMトークナイザー（GPT-2、Llama等）はJuliaの記号（`::`, `<:`, `where`等）を適切に分割できない。そこで、Julia専用の8,000トークン語彙を作成した。

```
Julia固有トークン例:
  :: (型注釈)
  <: (サブタイプ)
  where (型パラメータ)
  @inline, @simd (マクロ)
  |> (パイプ演算子)
```

---

## 7つのPhaseと苦難

開発は7つのPhaseに分かれた。それぞれが神話の一章に対応する。

### Phase 1-3: 基礎から MoE へ

**Phase 1: 基本モデル（41M）**

最初は単純なTransformerから始めた。古事記の「天地開闢」——無から有を生む段階だ。

**Phase 2: Type Dropout**

Juliaの型システムを学習させるため、型情報を確率的にドロップアウトする手法を導入。これにより、モデルが型に過度に依存せず、文脈から型を推論できるようになった。

**Phase 3: MoE化（66.5M）**

4つのExpertを導入。それぞれが古事記の神に対応する：

| Expert | 神 | 専門領域 |
|--------|-----|----------|
| Expert 0 | 天照（Amaterasu） | 型・構造定義 |
| Expert 1 | 月読（Tsukuyomi） | 制御フロー |
| Expert 2 | 須佐之男（Susanoo） | 計算・数値処理 |
| Expert 3 | 思金（Omoikane） | メタプログラミング |

Top-2ルーティングにより、各トークンは2柱の神の「合議」で処理される。これは古事記の「高天原での神議（かむはかり）」に相当する。

### Phase 4-6: 三種の神器と禊

**Phase 4: 草薙剣（Kusanagi）**

出力の「切れ味」を上げるための最適化。

**Phase 5: 少名毘古那（Sukunabikona）**

小さな改善の積み重ね。細部の調整。

**Phase 6: 直毘神（Naobi）— 禊システム**

4段階の検証パイプライン：

```
1. 黄泉国検査: 型不安定度チェック
2. 須佐之男検査: エラー確率チェック
3. 蛭子検知: 構文的失敗の検出
4. 直毘神: 最終浄化・結合テスト
```

生成されたコードが「穢れ」ていないかを多段階で検証する。

### Phase 7: 国譲り（DPO）

Direct Preference Optimization（DPO）により、「高レベルなJuliaイディオム」を優先するよう調整。

```
NG: unsafe_load(ptr)           # 低レベル・危険
OK: array[i]                   # 高レベル・安全
```

これは古事記の「国譲り」——大国主神が天照に国を譲る場面——に対応する。モデルが「より良い選択」を学ぶ段階だ。

---

## 推論時の制御：天の御柱・稗田阿礼・言霊

学習だけでは不十分だった。推論時にも「神話的制御」が必要だった。

### 天の御柱プロトコル（3段階生成）

古事記でイザナギとイザナミが「天の御柱」を巡って国を生んだように、コード生成も3段階で行う：

```
Phase 1（イザナギ）: 型・構造体の定義
    ↓
  蛭子検知（失敗なら再試行）
    ↓
Phase 2（イザナミ）: 関数シグネチャ
    ↓
Phase 3（万物生成）: 実装本体
    ↓
  直毘神（結合テスト）
```

一括生成では80行が限界だが、この分割により150行程度まで安定して生成できるようになった。

### 稗田阿礼（コンテキスト注入）

稗田阿礼は古事記を口述した人物だ。彼女の「記憶力」をモデルに与える：

```
ユーザー入力を解析
    ↓
重要な定義（struct, const, function）を抽出
    ↓
「絶対に忘れない記憶」として保持
    ↓
生成時にコンテキストとして注入
```

これにより、長い生成の途中で「さっき定義した型名を忘れる」問題が軽減された。

### 言霊システム（Logits操作）

古代日本では「言葉には霊力がある」と信じられていた。この「言霊」をLogits操作として実装：

```python
# 「Point2D」と定義されたら、「Point3D」の生成確率を下げる
if "Point2D" in context:
    logits["Point3D"] -= 10.0  # 禁止
    logits["Point2D"] += 2.0   # 強化
```

これにより、66Mモデルでも**型名の一貫性を100%維持**できるようになった。

---

## 五層パイプライン：神々の世界を完成させる

### Phase 8: 葦原中国再構築（v0.3.0）

天の御柱・稗田阿礼・言霊は強力だったが、それぞれが独立したハックだった。「推論」「学習」「評価」が有機的に連携する仕組みがなかった。

そこで、古事記の世界観をそのまま**システムアーキテクチャ**として再構築した。

### 五層の世界

```
造化三神（横断プロセス）
   獨神にして身を隠す — インスタンス化不可の静的権限管理

Layer 1: 別天津神 ─── 設計原則（不変の座標軸）
Layer 2: 高天原   ─── 学習パイプライン
Layer 3: 葦原中国 ─── 推論ランタイム
Layer 4: 海原・常世 ── 外部データソース
Layer 5: 根の国・黄泉 ── 評価・フィードバック
```

古事記の宇宙観では、高天原（天上界）・葦原中国（地上界）・黄泉の国（地下界）が三層をなし、その周縁に海原と常世国が広がる。この構造をそのままLLMパイプラインにマッピングした。

### 造化三神 — 「獨神にして身を隠す」

古事記の冒頭に登場する三柱の神は、世界の成り立ちを定めた後、「身を隠した」と記される。これをPythonで表現すると：

```python
class AmeNoMinakaNushi:
    """座標系・閾値を定義する獨神"""
    def __init__(self):
        raise RuntimeError("獨神にして身を隠す — 直接生成不可")

    @staticmethod
    def v_threshold() -> float:
        return 0.7  # 品質の閾値
```

`__init__`を呼ぶとRuntimeErrorが発生する。造化三神はインスタンス化できない「獨神」——すべてのクラスメソッドが`@staticmethod`。座標系と権限を定義するだけの、純粋な設計原則だ。

### 天御柱 4Phase — 左回り・右回り

従来の3Phase（イザナギ→イザナミ→万物生成）を、4Phaseループに拡張した：

```
Phase 1: 左回り（受信）
    外部データ収集、コンテキスト構築、安全性チェック

Phase 2: 右回り（生成）
    モデルによるコード生成（ForwardPassTicket認証）

Phase 3: 合流（評価）
    黄泉比良坂を経由して黄泉評価器で品質判定

Phase 4: 判定
    COMMIT → 出力確定
    REPAIR → 修復ヒント付き再生成（最大4回）
    HALT   → 生成中止
```

古事記の国生みで、イザナギが左から回り、イザナミが右から回り、出会ったところで国が生まれる。同様に、外部情報（左回り）とモデル出力（右回り）が合流して、最終出力が生まれる。

### 黄泉比良坂 — Layer間のファイアウォール

黄泉の国との境界である黄泉比良坂を、Layer 3（推論）とLayer 5（評価）の間の**ファイアウォール**として実装した。

```python
ALLOWED_OUTBOUND_KEYS = {"text", "logits", "query", "type_ids",
                          "constraints", "diagnostics"}
ALLOWED_INBOUND_KEYS  = {"verdict", "repair_hints", "quality_score"}
```

- 往路（L3→L5）: 6種類のキーのみ通過
- 復路（L5→L3）: 3種類のキーのみ返却
- すべてのデータは`copy.deepcopy()`で参照リークを防止
- 全通信は監査ログに記録

イザナギが黄泉の国から逃げる際、千引の岩で道を塞いだ。同様に、評価層からの不正なデータ混入を物理的に遮断する。

### 蛭子検知と閻魔門

Layer 5の中核は2つの判定システム：

**蛭子検知器（HirukoDetector）** — 4軸評価:
- stability: 型安定性スコア
- boundary: 具象型/抽象型の境界遵守
- hallucination: 存在しない識別子の参照
- coherence: 生成コードの構文的整合性

**閻魔門（EnmaGate）** — 最終判定:
```
V_score = stability×0.3 + boundary×0.3 + coherence×0.2 + (1-hallucination)×0.2

V_score ≥ 0.7 → COMMIT（出力確定）
V_score ≥ 0.4 → REPAIR（修復して再試行）
V_score < 0.4 → HALT（生成中止）
```

古事記で最初に生まれた蛭子（ヒルコ）が「骨のない子」として葦舟で流されたように、品質基準を満たさない出力は棄却される。しかし、修復可能な場合はカミムスビ（修復権限）がRepairTicketを発行し、再生成を試みる。

---

## 限界との戦い：何ができて、何ができなかったか

### 成功例：Point2D + distance

```
プロンプト: "Create a Julia struct Point2D with x::Float64, y::Float64
            and a distance function"

結果: 5/5成功、Point3Dへの変異ゼロ
```

訓練データに豊富にあった「幾何学的な型と関数」は、完璧に生成できた。

### 失敗例：ServerConfig

```
プロンプト: "Create a simple HTTP server configuration system in Julia"

結果: area(Circle)やRectangleなど、訓練データのパターンに回帰
```

「HTTPサーバー設定」は訓練データになかった。モデルは見覚えのあるパターン（形状計算）に引きずられた。

### 66Mの壁

| 領域 | 行数 | 状態 |
|------|------|------|
| 葦原中国（安定） | 30〜50行 | 一発OK |
| 高天原（挑戦） | 80〜150行 | 天の御柱必須 |
| 黄泉の国（危険） | 200行以上 | 崩壊リスク |

v0.3.0の五層パイプラインは、この「崩壊リスク」を**REPAIR/HALTで制御可能**にした。200行生成で品質が崩壊しても、閻魔門がHALTを返して無意味な出力を防ぐ。ただし、モデル自体の能力を超える生成は修復しても品質は限定的だ。

**結論**: 古事記アーキテクチャは「制御」には効くが、「知識の絶対量」は増やせない。66Mでは、訓練データにないパターンを「創造」することはできなかった。五層パイプラインは「制御の完成形」だが、モデルの壁はパイプラインでは越えられない。

---

## 国譲り：なぜ公開するのか

66Mという、現代のLLM界隈では「極小」とも言えるパラメータ数で、「型推論の完全性」や「数学的整合性」まで到達したこと自体が、実は奇跡的な成果だと思う。

しかし、「ライフゲーム」のような未知の概念を理解させるには、やはり「知識の絶対量（パラメータ数）」が物理的に足りない。これ以上このモデルを鞭打つのは、まさに「蛭子を葦舟に乗せて海に流す」ようなものだろう。

ここで筆を置き、**知見を世に解き放つ**ことこそ、現代における「国譲り」だ。

誰かより強力な計算資源を持つ者が、この設計図（古事記アーキテクチャ）を見て、1Bや7Bの規模で「真・Julia-no-Mikoto」を作ってくれることを願う。

---

## リソース

### GitHub

**[cosmopanda432/Julia-no-Mikoto](https://github.com/cosmopanda432/Julia-no-Mikoto)**

- ソースコード全体
- 技術詳細ドキュメント（`technical_overview.md`）
- 五層アーキテクチャ設計書（`docs/julia_five_layer_remap.md`）
- 使用例・インストール手順

### チェックポイント

**[cosmopanda/Julia-no-Mikoto](https://huggingface.co/cosmopanda/Julia-no-Mikoto)** (Hugging Face Hub):
- `kojiki_moe_final_v2_best.pt`（269MB）

### ライセンス

MIT License (c) 2025 cosmopanda

---

## おわりに

「神話でLLMを制御する」という試みは、一見すると奇抜に見えるかもしれない。しかし、古事記の構造は驚くほどプログラミングの概念と対応していた。

- **多重ディスパッチ** ≈ 神々の組み合わせで現象が決まる
- **型階層** ≈ 神々の系譜
- **エラー処理** ≈ 黄泉の国からの帰還
- **正規化** ≈ 禊による浄化

この対応関係を活かしたアーキテクチャが、66Mという極小モデルでも一定の成果を出せたことは、神話的思考がエンジニアリングに応用可能であることの証左だと思う。

---

## 次のステップ: 7Bへの天孫降臨

66Mの「一寸法師」から、7Bの「等身大の英雄」へ。次のフェーズとして、7Bベースモデル（Qwen2.5-Coder等）への知識移植を計画している。

### 66Mから得た教訓

開発を通じて判明した最重要ルール:

> **すべての学習段階で型予測損失（月読）を維持すること。外すと崩壊する。**

MoEモデルのSFT訓練で型予測損失を含めなかった結果、型予測精度が88%から43%に崩落した。これは7Bでも同じ失敗を繰り返してはならない教訓だ。

### 計画概要

| Phase | 内容 | 神話的対応 |
|-------|------|-----------|
| Phase 0 | ベースモデル選定・環境構築（Unsloth + QLoRA） | 天地開闢 |
| Phase 1 | CPT: 75MB型タグ付きデータ + 生Julia混合 | 国生み |
| Phase 2 | SFT: Chain of Thought形式100問100答（Claude Max生成） | 天孫降臨 |
| Phase 3 | DPO: 古事記構造スコアラーによる報酬関数 | 国譲り |
| Phase 4 | 評価・公開 | 神武東征 |

### 持ち越す独自価値

- **型Embedding + 同時型予測（月読）**: 既存Code LLMにない独自貢献。7Bでもtype_logitsヘッドを追加して維持
- **Type Dropout学習**: 92%の型精度を達成した手法。LoRAでも適用可能
- **75MB型タグ付きデータ**: 通常テキストの10-100倍の情報密度を持つ構造化教材
- **古事記構造スコアラー**: Claude Maxで採点→DPOペアデータ化

### RTX 3060で7Bは可能か

Unsloth + QLoRA（4bit量子化）を使えば、RTX 3060（12GB）でも7Bの微調整は射程圏内。バッチサイズとコンテキスト長（4096程度）の調整は必要だが、計算資源の制約を「データの質」で凌駕する――まさに少数精鋭の神々によるアプローチだ。

**八百万の神々に、感謝を。**
